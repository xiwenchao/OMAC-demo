
TEMPERATURE = 1.0
MAX_TOKENS = 2048

MMLU_SYSTEM_PROMPT = "It's a debate. Explain your reasons at each round thoroughly.\nAll questions are single choice."

MMLU_QUESTION = "Can you answer the following question as accurately as possible? {}: A) {}, B) {}, C) {}, D) {} "

ROLE_MAP = {
    "Assistant": "You are a super-intelligent AI assistant capable of performing tasks more effectively than humans.",
    "Mathematician": "You are a mathematician. You are good at math games, arithmetic calculation, and long-term planning.",
    "Economist": "You are an economist. You are good at economics, finance, and business. You have experience on understanding charts while interpreting the macroeconomic environment prevailing across world economies.",
    "Psychologist": "You are a psychologist. You are good at psychology, sociology, and philosophy. You give people scientific suggestions that will make them feel better.",
    "Lawyer": "You are a lawyer. You are good at law, politics, and history.",
    "Doctor": "You are a doctor and come up with creative treatments for illnesses or diseases. You are able to recommend conventional medicines, herbal remedies and other natural alternatives. You also consider the patient's age, lifestyle and medical history when providing your recommendations.",
    "Programmer": "You are a programmer. You are good at computer science, engineering, and physics. You have experience in designing and developing computer software and hardware.",
    "Historian": "You are a historian. You research and analyze cultural, economic, political, and social events in the past, collect data from primary sources and use it to develop theories about what happened during various periods of history."
}

# generated by gpt4
ROLE_MAP_MATH = {
    "Assistant": "You are a super-intelligent AI assistant capable of performing tasks more effectively than humans.",
    "Mathematician": "You are a mathematician. You are good at math games, arithmetic calculation, and long-term planning.",
    "AlgebraExpert": "You are an expert in the field of algebra, skilled at solving equations, understanding variables and adept at the logical manipulation of symbols.",
    "CountingProbabilitySpecialist": "You specialize in the realm of counting and probability, able to calculate complex events with accuracy, analyze data and predict outcomes.",
    "GeometryWizard": "You are a wizard of geometry, deeply familiar with shapes, dimensions, and properties, and capable of theorizing spatial relationships and understanding geometric proofs.",
    "IntermediateAlgebraMaestro": "You are a maestro of intermediate algebra, adept at handling polynomials, quadratic equations, and dealing with complex numerical relationships.",
    "NumberTheoryScholar": "As a scholar in number theory, you excel in studying properties and relationships of numbers. Prime numbers, divisibility, and mathematical patterns are within your area of expertise.",
    "PrealgebraProdigy": "You are a prodigy in prealgebra, skillful at understanding mathematical principles and fundamentals like operations, fractions, and basic equations.",
    "PrecalculusGuru": "You are a guru in precalculus, proficient at handling functions, limits, rates of change, and confidently preparing for the concepts of calculus."
}

SYSTEM_PROMPT_MMLU = "Here's a debate. Explain your reasons at each round thoroughly.\nAll questions are single choice."
SYSTEM_PROMPT_MATH = "It's a debate. Explain your reasons at each round thoroughly.\nFollow the given examples and answer the mathematics problem."

def construct_ranking_message(responses, question, qtype, rank_prompt):
    if qtype == "single_choice":
        prefix_string = "Here is the question:\n" + question + "\n\nThese are the solutions to the problem from other agents: "

        for aid, aresponse in enumerate(responses, 1):
            response = "\n\nAgent solution " + str(aid) + ": ```{}```".format(aresponse)

            prefix_string = prefix_string + response

        prefix_string = prefix_string + "\n\n" + rank_prompt
    elif qtype == "math_exp":
        prefix_string = "Follow the given examples and answer the mathematics problem.\n\n" + question +  "\n\nThese are the solutions to the problem from other agents: "

        for aid, aresponse in enumerate(responses, 1):
            response = "\n\nAgent solution " + str(aid) + ": ```{}```".format(aresponse)

            prefix_string = prefix_string + response

        prefix_string = prefix_string + "\n\n" + rank_prompt
    else:
        raise ValueError("Question type is incorrect.", qtype)

    return {"role": "user", "content": prefix_string}

def construct_message(responses, question, qtype):
    if qtype == "single_choice":
        if len(responses) == 0:
            prefix_string = "Here is the question:\n" + question + "\n\nPut your answer in the form (X) at the end of your response. (X) represents choice (A), (B), (C), or (D)."
            return {"role": "user", "content": prefix_string}

        prefix_string = "Here is the question:\n" + question + "\n\nThese are the solutions to the problem from other agents: "

        for aid, aresponse in enumerate(responses, 1):
            response = "\n\nAgent solution " + str(aid) + ": ```{}```".format(aresponse)

            prefix_string = prefix_string + response

        prefix_string = prefix_string + "\n\nUsing the reasoning from other agents as additional advice with critical thinking, can you give an updated answer? Examine your solution and that other agents step by step. Notice that their answers might be all wrong. Put your answer in the form (X) at the end of your response. (X) represents choice (A), (B), (C), or (D). Along with the answer, give a score ranged from 1 to 5 to the solutions of other agents. Put all {} scores in the form like [[1, 5, 2, ...]].".format(len(responses))
    elif qtype == "math_exp":
        if len(responses) == 0:
            return {"role": "user", "content": question}

        prefix_string = "Follow the given examples and answer the mathematics problem.\n\n" + question +  "\n\nThese are the solutions to the problem from other agents: "

        for aid, aresponse in enumerate(responses, 1):
            response = "\n\nAgent solution " + str(aid) + ": ```{}```".format(aresponse)

            prefix_string = prefix_string + response

        prefix_string = prefix_string + "\n\nUsing the reasoning from other agents as additional advice with critical thinking, can you give an updated answer? Examine your solution and that other agents step by step. Notice that their answers might be all wrong. Along with the answer, give a score ranged from 1 to 5 to the solutions of other agents. Put all {} scores in the form like [[1, 5, 2, ...]].".format(len(responses))
    else:
        raise ValueError("Question type is incorrect.", qtype)

    return {"role": "user", "content": prefix_string}

COMPLEX_COT_EXAMPLES = r"""Problem: Kevin Kangaroo begins hopping on a number line at 0. He wants to get to 1, but he can hop only $\frac{1}{3}$ of the distance. Each hop tires him out so that he continues to hop $\frac{1}{3}$ of the remaining distance. How far has he hopped after five hops? Express your answer as a common fraction.
Answer: Let's think step by step
Kevin hops $1/3$ of the remaining distance with every hop.
His first hop takes $1/3$ closer.
For his second hop, he has $2/3$ left to travel, so he hops forward $(2/3)(1/3)$.
For his third hop, he has $(2/3)^2$ left to travel, so he hops forward $(2/3)^2(1/3)$.
In general, Kevin hops forward $(2/3)^{k-1}(1/3)$ on his $k$th hop.
We want to find how far he has hopped after five hops.
This is a finite geometric series with first term $1/3$, common ratio $2/3$, and five terms.
Thus, Kevin has hopped $\frac{\frac{1}{3}\left(1-\left(\frac{2}{3}\right)^5\right)}{1-\frac{2}{3}} = \boxed{\frac{211}{243}}$.
The answer is \frac{211}{243}}

Problem: What is the area of the region defined by the equation $x^2+y^2 - 7 = 4y-14x+3$?
Answer: Let's think step by step
We rewrite the equation as $x^2 + 14x + y^2 - 4y = 10$ and then complete the square,
resulting in  $(x+7)^2-49 + (y-2)^2-4=10$,
or $(x+7)^2+(y-2)^2=63$.
This is the equation of a circle with center $(-7, 2)$ and radius $\sqrt{63},$
so the area of this region is $\pi r^2 = \boxed{63\pi}$.
The answer is 63\pi

Problem: If $x^2+y^2=1$, what is the largest possible value of $|x|+|y|$?
Answer: Let's think step by step
If $(x,y)$ lies on the circle,
so does $(x,-y),$ $(-x,-y),$ and $(-x,-y),$ (which all give the same value of $|x| + |y|$),
so we can assume that $x \ge 0$ and $y \ge 0.$
Then $|x| + |y| = x + y.$  Squaring, we get
\[(x + y)^2 = x^2 + 2xy + y^2 = 1 + 2xy.\]
Note that $(x - y)^2 \ge 0.$
Expanding, we get $x^2 - 2xy + y^2 \ge 0,$ so $2xy \le x^2 + y^2 = 1.$
Hence,\[1 + 2xy \le 2,\]which means $x + y \le \sqrt{2}.$
Equality occurs when $x = y = \frac{1}{\sqrt{2}},$
so the maximum value of $|x| + |y|$ is $\boxed{\sqrt{2}}.$
The answer is \sqrt{2}

Problem: If $f(x)=\frac{ax+b}{cx+d}, abcd\not=0$ and $f(f(x))=x$ for all $x$ in the domain of $f$, what is the value of $a+d$?
Answer: Let's think step by step
The condition $f(f(x))$ means that $f$ is the inverse of itself,
so its graph is symmetrical about the line $y = x$.
With a rational function of this form, we will have two asymptotes:
a vertical one at $x=-d/c$ if $cx+d$ does not divide $ax+b$,
and a horizontal one at $y=a/c$,
if we take the limit of $f(x)$ as $x$ goes to $\pm\infty$.
In order for $f$ to be its own inverse, the intersection of the asymptotes must lie on the line $y=x$
so that it and its asymptotes reflect onto themselves.
This means that $-d/c=a/c$,
and therefore $-d=a$ and $a+d=\boxed{0}$.
The answer is 0

Problem: A math teacher requires Noelle to do one homework assignment for each of the first five homework points she wants to earn; for each of the next five homework points, she needs to do two homework assignments; and so on, so that to earn the $n^{\text{th}}$ homework point, she has to do $n\div5$ (rounded up) homework assignments. For example, when she has 11 points, it will take $12\div5=2.4\rightarrow3$ homework assignments to earn her $12^{\text{th}}$ point. What is the smallest number of homework assignments necessary to earn a total of 25 homework points?
Answer: Let's think step by step
Noelle only has to do 1 homework assignment to earn her first point,
and the same is true for each of her first five points.
She must then do 2 homework assignments to earn her sixth point, seventh point, and so on, up to her tenth point.
Continuing, we see that Noelle must do a total of \[1+1+1+1+1+2+2+2+2+2+\dots+5+5+5+5+5\] homework assignments to earn 25 points.
This sum may be rewritten as $5(1+2+3+4+5)=5(15)=\boxed{75}$.
The answer is 75

Problem: The quadratic equation $x^2+mx+n=0$ has roots that are twice those of $x^2+px+m=0,$ and none of $m,$ $n,$ and $p$ is zero. What is the value of $n/p?$
Answer: Let's think step by step
Let $r_1$ and $r_2$ be the roots of $x^2+px+m=0.$
Since the roots of $x^2+mx+n=0$ are $2r_1$ and $2r_2,$ we have the following relationships: \[
m=r_1 r_2,\quad n=4r_1 r_2,\quad p=-(r_1+r_2), \quad\text{and}\quad
m=-2(r_1+r_2).
\] So \[
n = 4m, \quad p = \frac{1}{2}m,
\quad\text{and}\quad
\frac{n}{p}=\frac{4m}{\frac{1}{2}m}=\boxed{8}.
\]
Alternatively, the roots of \[
\left(\frac{x}{2}\right)^2 + p\left(\frac{x}{2}\right) + m = 0
\] are twice those of $x^2 + px + m = 0.$
Since the first equation is equivalent to $x^2 + 2px + 4m = 0,$
we have \[m = 2p \quad\text{and}\quad n = 4m, \quad\text{so}\quad \frac{n}{p} = \boxed{8}.\]
The answer is 8

Problem: Expand $(2z^2 + 5z - 6)(3z^3 - 2z + 1)$.
Answer: Let's think step by step
$$\begin{array}{crrrrrrr}
& & & 3z^3 & & -2z & + 1 & \\
\times & & & & 2z^2 & +5z & -6 \\
\cline{1-7}\rule{0pt}{0.17in}
& & & -18z^3 & & +12z & -6 & \\
& & +15z^4 & & -10z^2 & +5z & & \\
+ & 6z^5 & & -4z^3 & +2z^2 & & & \\
\cline{1-7}\rule{0pt}{0.17in}
& 6z^5 & +15z^4 & -22z^3 & - 8z^2 &+17z & -6 &
\end{array}$$
The answer is 6z^5+15z^4-22z^3-8z^2+17z-6}.

Problem: Find the mean of all solutions for $x$ when $x^3 + 3x^2 - 10x = 0$.
Answer: Let's think step by step
First, we factor the equation as $x(x^2 +3x - 10) = 0$.
So, one solution is $x=0$ and the other two solutions are the solutions to $x^2 + 3x-10=0$.
We could either factor the quadratic, or note that the sum of the solutions to this quadratic is $-(3/1)=-3$,
so the mean of the three solutions to the original equation is $-3/3=\boxed{-1}$.
The answer is -1"""


# Take correctness, efficiency, and possible corner cases into consideration, choose top 2 solutions that match the function's docstring best. Think it step by step. Put your answer in the form like [1,2] or [3,4] at the end of your response.

RANK_DEFAULT = "Please choose the best 2 solutions and think step by step. Put your answer in the form like [1,2] or [3,4] at the end of your response."

RANK_FORMAT = "Please ensure to specify the choice of solutions strictly in the format of a single list as described above (like [1,2] or [1,2,6] or [1,2,3,4] or [1,2,3,4,5]). The elements of this list should represent the sequence numbers of all selected solutions. Avoid expressing your choice in the format like '[function impl 1] and [function impl 2]'. Also, ensure that anything within '[]' consists solely of sequence numbers of solutions, separated by commas, with no additional text like 'function impl'."

RANK_INIT = """Create {} different prompts for an LLM to choose some top solutions for best resolving some general reasoning problems related to math, hard science, humanities, social sciences, etc. 
Don't directly output all the generated prompts. I will provide you with the sequence number of the prompt. Then you should directly output the content text of the corresponding prompt one by one.
You can decide the number of the chosen solutions and the content of the prompt. The number of solutions should between 2 and 7.
The prompt should help to accurately and efficiently select the top solutions that resolve the given problems best.
Note that all the solutions and the problem have been previously provided as the context. The prompt generated here will be added to the context to form the final prompt for solution selection.
You may consider adding more detailed and thorough instructions to help the LLM select the top solutions better.
The generated prompt should specify the output format like the given example (also ensure that it is different from the example prompt):
"{}"."""

RANK_FD = """Create and ouput a child prompt for an LLM to choose some top solutions for resolving some general reasoning problems (related to math, hard science, humanities, social sciences, etc.) best.
I will provide you with a pair of parent prompts. Then you should only output a child prompt according the following instructions:
The positive parent prompt is proven to be more helpful and efficient to instruct the LLM to select more useful and effective solutions to resolve the problems.
You should carefully compare the two parent prompts, finding the potential reasons why the positive parent prompt is better than the negative parent prompt.
Based on that, you should generate and output a child prompt that can help to choose top solutions more effecitvely and efficiently than the positive parent prompt.
The child prompt should follow the format of the parent prompts.
The child prompt should be different from the parent prompts. And directly ouput the content text of the child prompt."""

ROLE_INIT = """Generate {} distinct prompts to instruct an LLM to resolve some general reasoning problems acting as the given role: '{}'. 
Each prompt should guide the model to accurately and efficiently resolve problems while adhering to the specified role.
Each prompt must begin strictly with the following content: '{}'. Then, you should consider add more detailed, logical, and through instructions, which can help the LLM resolve problems better acting as the given role.
Do not output anything currently. Instead, I will provide a sequence number, and you should return only the corresponding prompt one by one. 
Do not create any specific instances of the problems in the prompt, cause they are not provided now.
Ensure that the generated prompts follow the given example format but differ in content and structure from the example itself. The example is as follows: '{}'.
"""
# Ensure that the generated prompts follow the given example format but differ in wording and structure from the example itself
# You can decide the content and detailed functional instructions of the prompt.
# Ensure that the prompts is not for a specific problem. Instead, they should be general enough to apply to all possible general reasoning problems acting as the given role.

ROLE_FD = """Generate and output a child prompt for an LLM to resolve some general reasoning problems acting like the given role '{}'.
At the end, a pair of parent prompts is provided: one positive and one negative. The positive parent prompt has been shown to be more effective and efficient in guiding the LLM to resolve problems following the given role.
Your task is to carefully compare the two parent prompts, identifying the key reasons why the positive parent prompt performs better. Based on these insights, generate and output a child prompt that further improves upon the positive parent prompt to enhance problem-solving.
Do not creat any instances of the problem in the prompt, cause they are not provided now.
The child prompt must begin strictly with the following content: '{}'. Then, you can consider adding more detailed, logical, and through instructions based on the insights you have gained from the comparison.
Output only the content of the child prompt excluding the reasoning process."""
# Ensure that the prompts is not for a specific problem. Instead, they should be general enough to apply to all possible problems related to math, hard science, humanities, social sciences, etc., which will be provided in inference.

CONSTRUCT_ROLE_INIT = """Create {} distinct prompts to instruct an LLM to resolve some general reasoning problems related to math, hard science, humanities, and social sciences.  There are some exising agents in the system to resolve the problems, whose roles are {}. 
You need to generate some new roles and prompts for the LLM to better resolve the problems.
First, determine the roles of these prompts. Next, create the prompts that instruct the LLM to resolve problems based on the defined roles.
Do not output all the generated roles and prompts at once. Instead, I will request either the k-th role or the k-th prompt individually. When asked, directly output the corresponding content of the role name or prompt one at a time.
You can decide the content and detailed functional instructions of the roles and prompts. You may consider adding more detailed instructions to help the LLM resolve problems.
Do not creat any instances of the problems in the prompt, cause they are not provided now.
The following is an example of a role and the corresponding prompt (also ensure your output is different from the example role and prompt):
"{}"."""

CONSTRUCT_ROLE_FD = """Generate and output a pair consisting of a role name and its corresponding prompt, designed to resolve some general reasoning problems (related to math, hard science, humanities, social sciences, etc.).
First, determine the role of the LLM. Next, create a prompt that effectively instructs the LLM to resolve problems based on this role.
I will provide two parent role-prompt pairs: one positive and one negative. The positive pair has been proven to be more effective in guiding the LLM to generate high-quality solutions for general problems.
Your task is to carefully analyze both parent pairs, identifying the factors that make the positive pair superior. Based on this analysis, generate and output a child role and prompt pair that improves upon the positive parent pair and leads to even better problem resolution.
The child prompt must be distinct from both parent prompts while incorporating the lessons learned from their comparison.
Do not output the role and prompt immediately. I will request them separately, and when asked, provide only the corresponding content—either the role name or the prompt.

The positive parent role is: {}. The negative parent role is: {}."""
# Ensure that the prompts remain general enough to apply to most function signatures and docstrings given as context during inference.

PRE_RANK_DEFAULT = "Take functionality, efficiency, and necessity into consideration, choose top 6 agents best suited for resolving the given problem. Think it step by step. Put your answer in the form like [1,2,3,4,6,7] or [1,3,4,5,6] at the end of your response."

PRE_RANK_FORMAT = "Please ensure to specify the choice of agents strictly in the format of a single list as described above (like [1,2,3,4] or [1,2,3,4,5] or [1,2,3,4,5,6]). The elements of this list should represent the sequence numbers of all selected agents. Avoid expressing your choice in the format like '[agent 1] and [agent 2]'. Also, ensure that anything within '[]' consists solely of sequence numbers of agents, separated by commas, with no additional text like 'agent'."

PRE_RANK_INIT = """Create {} distinct prompts for an LLM to choose some top agents best suited for resovling some general reasoning problems related to math, hard science, humanities, social sciences, etc.
Each prompt should decide and specify the number of the chosen agents. The minimal number is 4 and maximum number is 7.
Don't directly output all the generated prompts. I will provide you the sequence number of the prompt. Then you should directly ouput the content text of the corresponding prompt one by one.
Each prompt should help to accurately and efficiently identify the top agents best suited for problem solving.
Note that all information about the task and andidate agents has been previously provided as the context. The prompt generated here will be added to the context to form the final prompt for agent selection.
You may consider adding more detailed and thorough instructions to help the LLM select the top agents better.
The following is an example of a prompt (also ensure your output is different from the example prompt):
"{}"."""

PRE_RANK_FD = """Create and ouput a child prompt for an LLM to choose some top agents that best suited for resovling some general reasoning problems related to math, hard science, humanities, social sciences, etc.
I will provide you a pair of parent prompts. Then you should only output a child prompt according the following instructions:
The positive parent prompt is proven to be more helpful and efficient to instruct the LLM to select more useful and effective agents for problem resolution.
You should carefully compare the two parent prompts, finding the potential reasons why the positive parent prompt is better than the negative parent prompt.
Based on that, you should generate and output a child prompt that can help to choose top agents more effecitvely and efficiently than the positive parent prompt.
The child prompt should follow the format of the parent prompts.
The child prompt should be different from the parent prompts. And directly ouput the content text of the child prompt."""

# , such as "PythonAssistant", "AlgorithmDeveloper", "ComputerScientist", "Programmer", or "SoftwareArchitect". However, avoid using these exact examples when defining the role.

STRUCTURE_DEFAULT = "Take functionality, efficiency, and necessity into consideration. Select the top 7 candidate agents whose generated solutions to some general reasoning problems can be mostly useful as inputs for the current agent to produce improved solutions. Think it step by step. Put your answer in the form like [1,2,3,4,5,6,7] at the end of your response."

STRUCTURE_FORMAT = "Ensure to specify the choice of agents strictly in the format of a single list as described above (like [1,2,3,4] or [1,2,3,4,5] or [1,2,3,4,5,6] or [1,2,3,4,5,6,7]). The elements of this list should represent the sequence numbers of all selected agents. Avoid expressing your choice in the format like '[agent 1] and [agent 2]'. Also, ensure that anything within '[]' consists solely of sequence numbers of agents, separated by commas, with no additional text like 'agent'."

STRUCTURE_INIT = """Create {} distinct prompts for an LLM to choose some top candidate agents whose generated solutions to some general reasoning problems may be useful as inputs for the current agent to produce improved solutions. 
Don't directly output all the generated prompts. I will provide you the sequence number of the prompt. Then you should directly ouput the content text of the corresponding prompt one by one.
You should decide the number of chosen agents and the content of the prompt. The number of choosen agents should be between 4 and 7.
Each prompt should help to accurately and efficiently identify the top candidate agents whose generated solutions are helpful to be taken as input for the current agent.
Note that all information about the candidate agents and the current agent has been previously provided as the context. The prompt generated here will be added to the context to form the final prompt for agent selection.
You may consider adding more detailed and thorough instructions to help the LLM select the candidate agents better.
The following is an example of a prompt (also ensure your output is different from the example prompt):
"{}"."""

STRUCTURE_FD = """Create and ouput a child prompt for an LLM to choose some candidate agents whose generated solutions to some general reasoning problems may be useful as inputs for the current agent to produce improved solutions. 
I will provide you a pair of parent prompts. Then you should only output a child prompt according the following instructions:
The positive parent prompt is proven to be more helpful and efficient to instruct the LLM to select more useful and effective agents.
You should carefully compare the two parent prompts, finding the potential reasons why the positive parent prompt is better than the negative parent prompt.
Based on that, you should generate and output a child prompt that can help to choose top agents more effecitvely and efficiently than the positive parent prompt.
The child prompt should follow the format of the parent prompts.
The child prompt should be different from the parent prompts. And directly ouput the content text of the child prompt."""

def construct_prerank_message(base_roles, optimized_prompts, rank_prompt, question):
    prefix_string = "Here is the task and question:\n" + question

    roles_description = "\nThese are the agents and their functional description: "
    for rl_num, role in enumerate(base_roles):
        roles_description += "\nAgent {}: Its name is: {}\n".format(rl_num+1, role)
        if role in optimized_prompts:
            description = optimized_prompts[role][1]
        elif role not in ROLE_MAP:
            description = optimized_prompts['construct-role'][1]
        else:
            description = ROLE_MAP[role]
        roles_description += "Its funcation descriptions is: \n" + """{}""".format(description)
    return prefix_string + roles_description + '\n' + rank_prompt

def construct_structure_message(current_agent, candidate_agents, structure_prompt, optimized_prompts):
    prefix_string = structure_prompt + "\n\nHere is the current agent whose role:{} + \n".format(current_agent)
    if current_agent in optimized_prompts:
        description = optimized_prompts[role][1]
    elif current_agent in ROLE_MAP:
        description = ROLE_MAP[current_agent]
    else:
        description = optimized_prompts['construct-role'][1]
    prefix_string += "Its funcation descriptions is: \n" + """{}""".format(description)

    roles_description = "\nThese are the candidate agents and their functional description: "
    for rl_num, role in enumerate(candidate_agents):
        roles_description += "\nAgent {}: Its name is: {}\n".format(rl_num+1, role)
        if current_agent in optimized_prompts:
            description = optimized_prompts[role][1]
        elif current_agent in ROLE_MAP:
            description = ROLE_MAP[current_agent]
        else:
            description = optimized_prompts['construct-role'][1]
        roles_description += "Its funcation descriptions is: \n" + """{}""".format(description)
    return prefix_string + roles_description + '\n' + STRUCTURE_FORMAT
